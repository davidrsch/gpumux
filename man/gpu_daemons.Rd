% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/daemons.R
\name{gpu_daemons}
\alias{gpu_daemons}
\title{Create a mirai daemons object for GPU workers}
\usage{
gpu_daemons(
  gpu_ids = 0,
  n_workers = NULL,
  memory_per_worker_mb = NULL,
  reserve_memory_mb = 1024,
  framework = "none",
  worker_type = "persistent"
)
}
\arguments{
\item{gpu_ids}{A numeric vector of GPU IDs to use (e.g., \code{0}, \code{c(0, 1)}).}

\item{n_workers}{The total number of workers to create across all specified GPUs.
To terminate all daemons, set \code{n_workers = 0}.}

\item{memory_per_worker_mb}{The amount of VRAM in MB to allocate for each worker.
This is used for both capacity planning and, if a supported \code{framework} is
chosen, for setting a memory limit within the worker.}

\item{reserve_memory_mb}{A numeric value for the VRAM to reserve on each GPU in MB.}

\item{framework}{A character string specifying the ML/AI framework to be used
by the workers. Currently supported: \code{"none"} (default), \code{"tensorflow"}.
If a supported framework is specified, \code{gpumux} will automatically configure
the worker to respect the \code{memory_per_worker_mb} limit.}

\item{worker_type}{A character string specifying the daemon strategy.
\code{"persistent"} (the default) creates long-lived daemons that execute many
tasks, offering high performance. \code{"proxy"} creates daemons that spawn a
new, clean worker process for each task, offering maximum stability and
guaranteed memory cleanup at the cost of performance overhead.}
}
\value{
A \code{mirai} daemons object, ready to be used with \code{mirai::mirai()}.
}
\description{
This function intelligently creates a \code{mirai} daemons object by allocating
workers to specified GPUs based on available VRAM.
}
