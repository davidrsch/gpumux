[{"path":[]},{"path":"/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement daviddrsch@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to gpumux","title":"Contributing to gpumux","text":"outlines propose change kerasnip. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to gpumux","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES: edit roxygen comment .R file R/. : edit .Rd file man/.","code":""},{"path":"/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Contributing to gpumux","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reprex.","code":""},{"path":"/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"Contributing to gpumux","text":"recommend create Git branch pull request (PR). Look GitHub Actions build status making changes. README contains badges continuous integration services used package. New code follow tidyverse style guide. can use air package apply styles. can format code automatically commenting /style PR. use roxygen2, Markdown syntax, documentation. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to gpumux","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":[]},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 gpumux authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with gpumux","title":"Getting help with gpumux","text":"Thanks using gpumux. filing issue, places explore pieces put together make process smooth possible. Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site. Armed reprex, next step figure ask. ’s question: start community.posit.co, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed. right place, need file issue, please review “File issues” paragraph tidyverse contributing guidelines. Thanks help!","code":""},{"path":"/articles/gpumux-performance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"gpumux offers two distinct worker types, \"persistent\" \"proxy\", performance characteristics stability guarantees. vignette explores difference provides guidance choose workload. persistent (Default): worker type creates long-lived R sessions persist tasks. offers highest performance avoids overhead starting new R process computation. ideal tasks well-behaved manage GPU memory effectively. proxy: worker type creates lightweight “proxy” daemon , turn, spawns completely new, ephemeral R process every single task. provides ultimate stability guarantees resources (including VRAM) released back OS task complete. perfect solution preventing memory leaks frameworks like TensorFlow PyTorch, incurs performance penalty due process startup overhead. run benchmark, need tensorflow processx packages installed, along CUDA-enabled NVIDIA GPU.","code":"library(gpumux) library(mirai) library(tensorflow) library(microbenchmark) library(purrr)  # Check if a GPU is available gpu_devices <- gpumux::list_gpus() can_run_benchmark <- nrow(gpu_devices) > 0 if (!can_run_benchmark) {   message(\"No GPU devices detected. Skipping benchmark.\") }"},{"path":"/articles/gpumux-performance.html","id":"task-definitions","dir":"Articles","previous_headings":"","what":"Task Definitions","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"illustrate performance trade-offs, define three types tasks: Heavy Task: large matrix multiplication (2048x2048) designed saturate GPU. Medium Task: smaller matrix multiplication (1024x1024) saturate GPU. Light Task: trivial operation returns instantly highlight overhead.","code":"# 1. Heavy Task: Large matrix multiplication heavy_task <- function(device = \"gpu\") {   library(tensorflow)   device_str <- if (device == \"gpu\") \"/gpu:0\" else \"/cpu:0\"   with(tf$device(device_str), {     x <- tf$random$normal(shape(2048, 2048))     y <- tf$linalg$matmul(x, x)     as.numeric(tf$reduce_sum(y))   }) }  # 2. Medium-Workload Task medium_task <- function(device = \"gpu\") {   library(tensorflow)   device_str <- if (device == \"gpu\") \"/gpu:0\" else \"/cpu:0\"   with(tf$device(device_str), {     x <- tf$random$normal(shape(1024, 1024)) # Smaller matrix     y <- tf$linalg$matmul(x, x)     as.numeric(tf$reduce_sum(y))   }) }  # 3. Light Task: Returns a simple value instantly light_task <- function() {   return(Sys.time()) }"},{"path":"/articles/gpumux-performance.html","id":"benchmark-1-gpu-saturating-tasks-the-stress-test","dir":"Articles","previous_headings":"","what":"Benchmark 1: GPU-Saturating Tasks (The “Stress Test”)","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"first benchmark uses heavy task. represents “worst-case” scenario parallelism, tasks compete heavily hardware. Note Memory Cleanup: may notice benchmark, VRAM fully cleared. Sequential GPU - Heavy run happens main R process. TensorFlow known hold onto VRAM R session terminated. perfect illustration resource management problem gpumux solves. gpumux runs tasks separate daemon processes, can guarantee resources released daemons terminated.","code":"# Function to add total time to summary summary_with_total <- function(bench_data) {   summary_df <- summary(bench_data)   total_time_ns <- sum(bench_data$time)   summary_df$total_time <- total_time_ns / 1e9    return(summary_df) }  # --- 1. Persistent Worker Benchmark (Heavy) --- message(\"Running HEAVY benchmark with 'persistent' workers...\") gpumux::gpu_daemons(   n_workers = 4,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"persistent\" ) persistent_heavy_bench <- microbenchmark(   \"Persistent - Heavy\" = {     results <- rep(\"gpu\", 4) |>        map(in_parallel(\\(x) heavy_task(x), heavy_task = heavy_task))     results   },   times = 5L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 2. Proxy Worker Benchmark (Heavy) --- message(\"Running HEAVY benchmark with 'proxy' workers...\") gpumux::gpu_daemons(   n_workers = 4,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"proxy\" ) proxy_heavy_bench <- microbenchmark(   \"Proxy - Heavy\" = {     results <- rep(\"gpu\", 4) |>       map(in_parallel(\\(x) heavy_task(x), heavy_task = heavy_task))     results   },   times = 5L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 3. Sequential GPU Benchmark (Heavy) --- message(\"Running HEAVY benchmark with 'Sequential GPU' (via single worker)...\") # By running the tasks on a single worker daemon, we simulate sequential # execution while still benefiting from the process isolation and automatic # cleanup that gpumux provides. gpumux::gpu_daemons(   n_workers = 1,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"persistent\" ) sequential_gpu_heavy_bench <- microbenchmark(   \"Sequential GPU - Heavy\" = {     # With only one worker, these 4 tasks will be processed sequentially.     results <- rep(\"gpu\", 4) |>        map(in_parallel(\\(x) heavy_task(x), heavy_task = heavy_task))     results   },   times = 5L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 4. CPU Benchmark (Heavy) --- message(\"Running HEAVY benchmark with 'CPU' workers...\") mirai::daemons(4) cpu_heavy_bench <- microbenchmark(   \"CPU - Heavy\" = {     results <- rep(\"cpu\", 4) |>       map(in_parallel(\\(x) heavy_task(x), heavy_task = heavy_task))     results   },    times = 5L,   unit = \"seconds\" ) mirai::daemons(0)  # --- 5. Results (Heavy) --- heavy_benchmarks <- rbind(   summary_with_total(persistent_heavy_bench),   summary_with_total(proxy_heavy_bench),   summary_with_total(sequential_gpu_heavy_bench),   summary_with_total(cpu_heavy_bench) ) knitr::kable(heavy_benchmarks, caption = \"GPU-Saturating Workload Results\")"},{"path":"/articles/gpumux-performance.html","id":"benchmark-2-non-saturating-gpu-tasks-the-sweet-spot","dir":"Articles","previous_headings":"","what":"Benchmark 2: Non-Saturating GPU Tasks (The “Sweet Spot”)","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"benchmark uses medium task. task doesn’t use 100% GPU, spare capacity gpumux leverage.","code":"# --- 1. Persistent Worker Benchmark (Medium) --- message(\"Running MEDIUM benchmark with 'persistent' workers...\") gpumux::gpu_daemons(   n_workers = 4,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"persistent\" ) persistent_medium_bench <- microbenchmark(   \"Persistent - Medium\" = {     results <- rep(\"gpu\", 4) |>       map(in_parallel(\\(x) medium_task(x), medium_task = medium_task))     results   },   times = 10L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 2. Proxy Worker Benchmark (Medium) --- message(\"Running MEDIUM benchmark with 'proxy' workers...\") gpumux::gpu_daemons(   n_workers = 4,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"proxy\" ) proxy_medium_bench <- microbenchmark(   \"Proxy - Medium\" = {     results <- rep(\"gpu\", 4) |>       map(in_parallel(\\(x) medium_task(x), medium_task = medium_task))     results   },   times = 10L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 3. Sequential GPU Benchmark (Medium) --- message(\"Running MEDIUM benchmark with 'Sequential GPU' (via single worker)...\") gpumux::gpu_daemons(   n_workers = 1,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"persistent\" ) sequential_medium_bench <- microbenchmark(   \"Sequential GPU - Medium\" = {     results <- rep(\"gpu\", 4) |>        map(in_parallel(\\(x) medium_task(x), medium_task = medium_task))     results   },   times = 10L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 4. CPU Benchmark (Medium) --- message(\"Running MEDIUM benchmark with 'CPU' workers...\") mirai::daemons(4) cpu_medium_bench <- microbenchmark(   \"CPU - Medium\" = {     results <- rep(\"cpu\", 0) |>       map(in_parallel(\\(x) medium_task(x), medium_task = medium_task))     results   },   times = 10L,   unit = \"seconds\" ) mirai::daemons(0)  # --- 5. Results (Medium) --- medium_benchmarks <- rbind(   summary_with_total(persistent_medium_bench),   summary_with_total(proxy_medium_bench),   summary_with_total(sequential_medium_bench),   summary_with_total(cpu_medium_bench) ) knitr::kable(medium_benchmarks, caption = \"Non-Saturating Workload Results\")"},{"path":"/articles/gpumux-performance.html","id":"benchmark-3-trivial-tasks-the-overhead-test","dir":"Articles","previous_headings":"","what":"Benchmark 3: Trivial Tasks (The “Overhead Test”)","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"Finally, benchmark uses light task isolate measure raw overhead different execution methods.","code":"# --- 1. Persistent Worker Benchmark (Light) --- message(\"Running LIGHT benchmark with 'persistent' workers...\") gpumux::gpu_daemons(   n_workers = 4,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"persistent\" ) persistent_light_bench <- microbenchmark(   \"Persistent - Light\" = {     results <- rep(\"gpu\", 4) |>       map(in_parallel(\\(x) light_task(), light_task = light_task))     results   },   times = 20L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 2. Proxy Worker Benchmark (Light) --- message(\"Running LIGHT benchmark with 'proxy' workers...\") gpumux::gpu_daemons(   n_workers = 4,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"proxy\" ) proxy_light_bench <- microbenchmark(   \"Proxy - Light\" = {     results <- rep(\"gpu\", 4) |>       map(in_parallel(\\(x) light_task(), light_task = light_task))     results   },   times = 20L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 3. Sequential GPU Benchmark (Light) --- message(\"Running LIGHT benchmark with 'Sequential GPU' (via single worker)...\") gpumux::gpu_daemons(   n_workers = 1,   gpu_ids = 0,   memory_per_worker_mb = 1024,   framework = \"tensorflow\",   worker_type = \"persistent\" ) sequential_light_bench <- microbenchmark(   \"Sequential GPU - Light\" = {     results <- rep(\"gpu\", 4) |>        map(in_parallel(\\(x) light_task(), light_task = light_task))     results   },   times = 20L,   unit = \"seconds\" ) gpumux::gpu_daemons(n_workers = 0)  Sys.sleep(3)  # --- 4. CPU Benchmark (Light) --- message(\"Running LIGHT benchmark with 'CPU'...\") mirai::daemons(4) cpu_light_bench <- microbenchmark(   \"CPU - Light\" = {     results <- rep(\"cpu\", 4) |>       map(in_parallel(\\(x) light_task(), light_task = light_task))     results   },   times = 20L,   unit = \"seconds\" ) mirai::daemons(0)  # --- 5. Results (Light) --- light_benchmarks <- rbind(   summary_with_total(persistent_light_bench),   summary_with_total(proxy_light_bench),   summary_with_total(sequential_light_bench),   summary_with_total(cpu_light_bench) ) knitr::kable(light_benchmarks, caption = \"Light Workload (Overhead) Results\")"},{"path":"/articles/gpumux-performance.html","id":"benchmark-4-oom-stability-test","dir":"Articles","previous_headings":"","what":"Benchmark 4: OOM Stability Test","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"benchmark uses oom_task simulate memory leak. use fixed number iterations designed consume approximately 3GB VRAM, enough trigger --Memory (OOM) error stateful workers (persistent sequential) stateless proxy worker. provides direct, quantitative comparison worker stability.","code":"# 4. OOM Task: Deliberately allocates memory in a loop to simulate a leak. oom_task <- function(device = \"gpu\") {   library(tensorflow)   device_str <- if (device == \"gpu\") \"/gpu:0\" else \"/cpu:0\"   with(tf$device(device_str), {     for (i in seq_len(30)) {       x <- tf$random$normal(shape(2048, 2048))       y <- tf$linalg$matmul(x, x)       as.numeric(tf$reduce_sum(y))     }   }) } # We will attempt to allocate ~3GB of VRAM (30 iterations * 100MB).  # --- 1. Persistent Worker OOM Benchmark --- message(\"\\n--- Running OOM Benchmark for 'persistent' worker ---\\n\") persistent_oom_bench <- tryCatch({   gpumux::gpu_daemons(     n_workers = 4,     gpu_ids = 0,     memory_per_worker_mb = 1024,     framework = \"tensorflow\",     worker_type = \"persistent\"   )   bench <- microbenchmark(     \"Persistent - OOM\" = {       # With only one worker, these 4 tasks will be processed sequentially.       results <- rep(\"gpu\", 4) |>          map(in_parallel(\\(x) oom_task(x), oom_task = oom_task))       results     },     times = 5L,     unit = \"seconds\"   )   gpumux::gpu_daemons(n_workers = 0)   summary_df <- summary_with_total(bench)   Sys.sleep(3)   summary_df }, error = function(e) {   message(\"Persistent worker failed as expected.\")   gpumux::gpu_daemons(n_workers = 0)   Sys.sleep(3)   data.frame(expr = \"Persistent - OOM\", min = NA, lq = NA, mean = NA, median = NA, uq = NA, max = NA, neval = 0, total_time = NA) })   # --- 2. Proxy Worker OOM Benchmark --- message(\"\\n--- Running OOM Benchmark for 'proxy' worker ---\\n\") proxy_oom_bench <- tryCatch({   gpumux::gpu_daemons(     n_workers = 4,     gpu_ids = 0,     memory_per_worker_mb = 1024,     framework = \"tensorflow\",     worker_type = \"proxy\"   )   bench <- microbenchmark(     \"Proxy - OOM\" = {       # With only one worker, these 4 tasks will be processed sequentially.       results <- rep(\"gpu\", 4) |>          map(in_parallel(\\(x) oom_task(x), oom_task = oom_task))       results     },     times = 5L,     unit = \"seconds\"   )   gpumux::gpu_daemons(n_workers = 0)   summary_df <- summary_with_total(bench)   Sys.sleep(3)   summary_df }, error = function(e) {   message(\"Proxy worker failed unexpectedly!\")   gpumux::gpu_daemons(n_workers = 0)   Sys.sleep(3)   data.frame(expr = \"Proxy - OOM\", min = NA, lq = NA, mean = NA, median = NA, uq = NA, max = NA, neval = 0, total_time = NA) })   # --- 3. Sequential GPU OOM Benchmark --- message(\"\\n--- Running OOM Benchmark for 'Sequential GPU' worker ---\\n\") sequential_oom_bench <- tryCatch({   gpumux::gpu_daemons(     n_workers = 1,     gpu_ids = 0,     memory_per_worker_mb = 4096,     framework = \"tensorflow\",     worker_type = \"persistent\"   )   bench <- microbenchmark(     \"Sequential GPU - OOM\" = {       results <- rep(\"gpu\", 4) |>          map(in_parallel(\\(x) oom_task(x), oom_task = oom_task))       results     },     times = 5L,     unit = \"seconds\"   )   summary_df <- summary_with_total(bench)   gpumux::gpu_daemons(n_workers = 0)   Sys.sleep(3)   summary_df }, error = function(e) {   message(\"Sequential GPU worker failed as expected.\")   gpumux::gpu_daemons(n_workers = 0)   Sys.sleep(3)   data.frame(expr = \"Sequential GPU - OOM\", min = NA, lq = NA, mean = NA, median = NA, uq = NA, max = NA, neval = 0, total_time = NA) })   # --- 4. Results (OOM) --- oom_benchmarks <- rbind(   persistent_oom_bench,   proxy_oom_bench,   sequential_oom_bench ) knitr::kable(oom_benchmarks, caption = \"OOM Stability Test Results\")"},{"path":"/articles/gpumux-performance.html","id":"conclusion-and-recommendations","dir":"Articles","previous_headings":"","what":"Conclusion and Recommendations","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"three benchmarks tell clear story performance stability: GPU-Saturating Workload, sequential execution fastest. task demanding needs entire GPU. Running parallel creates resource competition slows whole process . benchmark also highlights significant performance overhead proxy worker, price high degree stability. Non-Saturating Workload, persistent parallel workers significantly faster. task used fraction GPU’s power, gpumux able run truly concurrently, leading significant throughput gain. performance “sweet spot” package. Light Workload, running CPU far fastest. demonstrates trivial tasks, overhead sending task kind worker (even persistent one) much greater execution time task .","code":""},{"path":"/articles/gpumux-performance.html","id":"key-takeaways","dir":"Articles","previous_headings":"Conclusion and Recommendations","what":"Key Takeaways:","title":"Advanced Usage: Worker Types and Performance Trade-offs","text":"gpumux shines tasks don’t saturate GPU. main performance benefit comes using GPU’s spare capacity run multiple tasks . gpumux provides critical resource management. seen sequential benchmark, running GPU tasks main R session can lead memory leaks. gpumux solves isolating tasks worker processes, guaranteeing cleanup. Choose worker type wisely. proxy worker provides maximum stability cost performance. persistent worker provides maximum performance requires tasks well-behaved. small tasks, overhead worker type can substantial. Know workload. parallelizing, understand tasks heavy enough saturate hardware, idle periods, trivial. nvidia-smi command line tool great way observe GPU utilization single task run.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"First Last. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Last F (2025). gpumux: GPU Task Multiplexer Mirai. R package version 0.0.0.9000.","code":"@Manual{,   title = {gpumux: GPU Task Multiplexer for Mirai},   author = {First Last},   year = {2025},   note = {R package version 0.0.0.9000}, }"},{"path":"/index.html","id":"gpumux","dir":"","previous_headings":"","what":"GPU Task Multiplexer for Mirai","title":"GPU Task Multiplexer for Mirai","text":"goal gpumux provide VRAM-aware worker management safely run multiple mirai daemons one GPUs. supports high-performance persistent workers stable proxy workers isolate tasks separate processes. package can also proactively initialize workers specific deep learning frameworks, configuring GPU memory limits automatically.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"GPU Task Multiplexer for Mirai","text":"can install development version gpumux GitHub :","code":"# install.packages(\"pak\") pak::pak(\"davidrsch/gpumux\")"},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"GPU Task Multiplexer for Mirai","text":"example shows use gpumux create mirai daemons available GPUs.","code":"library(gpumux) library(mirai)  # 1. List available GPUs # This function is vendor-agnostic and will be improved to support more vendors in the future. # For now, it supports NVIDIA GPUs. gpus <- list_gpus() print(gpus)  # 2. Create daemons on the first GPU # This will create 2 workers on GPU 0, allocating 1GB of VRAM for each. # It will also reserve 1GB of VRAM on the GPU. if (nrow(gpus) > 0) {   daemons <- gpu_daemons(     gpu_ids = 0,     n_workers = 2,     memory_per_worker_mb = 1024,     reserve_memory_mb = 1024   ) }  # 3. Use the daemons with mirai # ... your mirai code here ...  # 4. Terminate the daemons when you are done if (exists(\"daemons\")) {   daemons(0) }"},{"path":"/reference/gpu_daemons.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a mirai daemons object for GPU workers — gpu_daemons","title":"Create a mirai daemons object for GPU workers — gpu_daemons","text":"function intelligently creates mirai daemons object allocating workers specified GPUs based available VRAM.","code":""},{"path":"/reference/gpu_daemons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a mirai daemons object for GPU workers — gpu_daemons","text":"","code":"gpu_daemons(   gpu_ids = 0,   n_workers = NULL,   memory_per_worker_mb = NULL,   reserve_memory_mb = 1024,   framework = \"none\",   worker_type = \"persistent\" )"},{"path":"/reference/gpu_daemons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a mirai daemons object for GPU workers — gpu_daemons","text":"gpu_ids numeric vector GPU IDs use (e.g., 0, c(0, 1)). n_workers total number workers create across specified GPUs. terminate daemons, set n_workers = 0. memory_per_worker_mb amount VRAM MB allocate worker. used capacity planning , supported framework chosen, setting memory limit within worker. reserve_memory_mb numeric value VRAM reserve GPU MB. framework character string specifying ML/AI framework used workers. Currently supported: \"none\" (default), \"tensorflow\". supported framework specified, gpumux automatically configure worker respect memory_per_worker_mb limit. worker_type character string specifying daemon strategy. \"persistent\" (default) creates long-lived daemons execute many tasks, offering high performance. \"proxy\" creates daemons spawn new, clean worker process task, offering maximum stability guaranteed memory cleanup cost performance overhead.","code":""},{"path":"/reference/gpu_daemons.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a mirai daemons object for GPU workers — gpu_daemons","text":"mirai daemons object, ready used mirai::mirai().","code":""},{"path":"/reference/list_gpus.html","id":null,"dir":"Reference","previous_headings":"","what":"List available GPUs and their memory status from all supported vendors. — list_gpus","title":"List available GPUs and their memory status from all supported vendors. — list_gpus","text":"function queries system GPUs supported vendors (currently NVIDIA) reports identity current memory allocation.","code":""},{"path":"/reference/list_gpus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available GPUs and their memory status from all supported vendors. — list_gpus","text":"","code":"list_gpus()"},{"path":"/reference/list_gpus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available GPUs and their memory status from all supported vendors. — list_gpus","text":"data.frame one row per GPU, containing following columns: gpu_id (integer): 0-indexed ID GPU. vendor (character): GPU vendor (e.g., \"nvidia\"). name (character): product name GPU (e.g., \"NVIDIA GeForce RTX 3090\"). memory_total_mb (numeric): total installed memory GPU megabytes. memory_free_mb (numeric): currently available memory GPU megabytes.","code":""},{"path":"/reference/list_gpus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List available GPUs and their memory status from all supported vendors. — list_gpus","text":"","code":"if (FALSE) { # \\dontrun{   list_gpus() } # }"},{"path":"/news/index.html","id":"gpumux-development-version","dir":"Changelog","previous_headings":"","what":"gpumux (development version)","title":"gpumux (development version)","text":"Initial release.","code":""}]
